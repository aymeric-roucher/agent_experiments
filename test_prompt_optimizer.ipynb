{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import os\n",
    "import plotly.express as px\n",
    "import json\n",
    "import os\n",
    "\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chat_models import ChatHuggingFace, ChatOpenAI\n",
    "\n",
    "from scripts.optimize_prompt import optimize_prompt, get_answers\n",
    "from scripts.evaluation import (\n",
    "    load_math_datasets,\n",
    "    score_last_match_series,\n",
    ")\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math_dataset = load_math_datasets()\n",
    "\n",
    "eval_dataset, fewshot_dataset, validation_dataset = (\n",
    "    math_dataset.select(range(30)),\n",
    "    math_dataset.select(range(30, 35)),\n",
    "    math_dataset.select(range(35, 50)),\n",
    ")\n",
    "eval_df, fewshot_df, validation_df = (\n",
    "    pd.DataFrame(eval_dataset),\n",
    "    pd.DataFrame(fewshot_dataset),\n",
    "    pd.DataFrame(validation_dataset),\n",
    ")\n",
    "\n",
    "OUTPUT_DIR = \"dump\"\n",
    "prompts_file_name = f\"{OUTPUT_DIR}/prompts_gsm8k_mixtral.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_COT = False\n",
    "USE_FEW_SHOT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup student agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "llm_client = InferenceClient(\n",
    "    model=\"HuggingFaceH4/zephyr-7b-beta\",\n",
    "    timeout=120,\n",
    ")\n",
    "llm_client.text_generation(prompt=\"How are you today?\", max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup teacher optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_agent = ChatOpenAI(model=\"gpt-4-1106-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "Q: What is the answer to the following math problem? Make sure to think first, then give your answer at the end in the format \"The answer is 42.36\".\n",
    "- {question}\n",
    "\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_logs = []\n",
    "logs = optimize_prompt(\n",
    "    initial_logs,\n",
    "    initial_prompt,\n",
    "    validation_df,\n",
    "    llm_client,\n",
    "    teacher_agent=teacher_agent,\n",
    "    scoring_function=score_last_match_series,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"{OUTPUT_DIR}/optimizer_zephyr_gpt4-teach4.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(file_path):\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        pickle.dump(logs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs = pickle.load(open(file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs[0][\"answers\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best_prompt = max(enumerate(logs), key=(lambda x: x[1][\"score\"]))[0]\n",
    "best_prompt = logs[index_best_prompt][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[(el[\"prompt\"], el[\"score\"]) for el in logs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test all prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = {\n",
    "    \"initial_prompt\": {\"prompt\": initial_prompt},\n",
    "    \"best_prompt\": {\"prompt\": best_prompt},\n",
    "    \"CoT\": {\"prompt\": initial_prompt + \" Let's think step-by-step. \"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_prompt = \"\"\"\n",
    "Please answer the following math problem. Make sure to give your answer as a float, and the LAST NUMBER OF ALL NUMBER YOU GIVE in the format \"The answer is 42.36 dollars\".\n",
    "Here are a few examples to help you.\n",
    "\"\"\"\n",
    "for example in eval_dataset.select(range(3)):\n",
    "    fewshot_prompt += f\"\"\"\n",
    "Q: {example['question']}\n",
    "A: {example['true_reasoning'] + '. So the answer is ' + str(example['true_answer'])}\n",
    "\"\"\"\n",
    "fewshot_prompt += \"Now begin!\\nQ: {question}\\n\\nA:\"\n",
    "fewshot_cot_prompt = fewshot_prompt + \"Let's think step-by-step.\"\n",
    "\n",
    "prompt_dict[\"fewshot\"] = {\"prompt\": fewshot_prompt}\n",
    "prompt_dict[\"fewshot_cot\"] = {\"prompt\": fewshot_cot_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt_name, values in prompt_dict.items():\n",
    "    if \"score\" not in values:\n",
    "        prompt = values[\"prompt\"]\n",
    "        print(f\"========== Prompt: {prompt_name} ==========\")\n",
    "        print(f\"Prompt content: {prompt}\")\n",
    "        answers = get_answers(prompt, llm_client, eval_df[\"question\"])\n",
    "        eval_df[\"is_correct\"] = score_last_match_series(answers, eval_df[\"true_answer\"])\n",
    "        prompt_dict[prompt_name][\"answers\"] = answers.to_dict()\n",
    "        prompt_dict[prompt_name][\"score\"] = eval_df[\"is_correct\"].mean()\n",
    "        print(eval_df[\"is_correct\"].mean())\n",
    "\n",
    "prompt_dict[\"langchain_agent\"] = {\n",
    "    \"prompt\": \"Cf source file\",\n",
    "    \"answers\": \"cf other experiment\",\n",
    "    \"score\": 0.73,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_file_name = \"dump/prompts_gsm8k_zephyr_gpt4-teach4.json\"\n",
    "if not os.path.exists(prompts_file_name):\n",
    "    with open(prompts_file_name, \"w\") as f:\n",
    "        json.dump(prompt_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_mistral = \"dump/prompts_gsm8k_zephyr_gpt4-teach4.json\"\n",
    "file_name_mixtral = \"dump/prompts_gsm8k_mixtral.json\"\n",
    "\n",
    "prompt_dict_mistral = json.load(open(file_name_mistral, \"r\"))\n",
    "prompt_dict_mixtral = json.load(open(file_name_mixtral, \"r\"))\n",
    "prompt_dict_mistral.pop(\"langchain_agent\", None)\n",
    "prompt_dict_mixtral.pop(\"langchain_agent\", None)\n",
    "prompt_dict_mixtral.pop(\"fewshot\", None)\n",
    "prompt_dict_mistral.pop(\"fewshot\", None)\n",
    "\n",
    "results_df_mistral = pd.DataFrame(\n",
    "    [\n",
    "        {**{\"prompt_name\": key, \"model\": \"mistral-7b\"}, **value}\n",
    "        for key, value in prompt_dict_mistral.items()\n",
    "    ]\n",
    ")\n",
    "results_df_mixtral = pd.DataFrame(\n",
    "    [\n",
    "        {**{\"prompt_name\": key, \"model\": \"mixtral-8x7b\"}, **value}\n",
    "        for key, value in prompt_dict_mixtral.items()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = results_df_mixtral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df_mistral, results_df_mixtral])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = results_df.groupby([\"prompt_name\", \"model\"])[[\"score\"]].mean().reset_index()\n",
    "aggregate[\"score\"] = aggregate[\"score\"] * 100\n",
    "aggregate = aggregate.sort_values(\"score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate[\"prompt_name\"] = aggregate[\"prompt_name\"].map(\n",
    "    {\n",
    "        \"initial_prompt\": \"Initial prompt\",\n",
    "        \"best_prompt\": \"Optimized prompt\",\n",
    "        \"CoT\": \"CoT\",\n",
    "        \"fewshot\": \"Fewshot\",\n",
    "        \"fewshot_cot\": \"Fewshot+CoT\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(\n",
    "    aggregate,\n",
    "    x=\"prompt_name\",\n",
    "    color=\"model\",\n",
    "    y=\"score\",\n",
    "    labels={\n",
    "        \"prompt_name\": \"<b>Prompt choice</b>\",\n",
    "        \"score\": \"<b>Score</b>\",\n",
    "        \"fewshot\": \"Few-shot\",\n",
    "    },\n",
    ")\n",
    "fig.update_layout(\n",
    "    width=aggregate[\"prompt_name\"].nunique() * 100 + 200,\n",
    "    height=600,\n",
    "    barmode=\"group\",\n",
    "    bargap=0.35,\n",
    "    bargroupgap=0.0,\n",
    "    yaxis_range=[0, 80],\n",
    ")\n",
    "fig.update_traces(texttemplate=\"%{y:.0f}\", textposition=\"outside\")\n",
    "fig.layout.yaxis.ticksuffix = \"%\"\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from the experiment\n",
    "- Prompt optimization with GPT4 does not seem to work well for big models like Mixtral 🚫\n",
    "- Prompting techniques are most important for less powerful models like Mistral-7B"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
