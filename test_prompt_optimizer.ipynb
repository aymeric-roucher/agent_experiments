{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "pd.set_option(\"max_colwidth\", None)\n",
    "\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.chat_models import ChatHuggingFace\n",
    "\n",
    "from scripts.run_agents import answer_questions\n",
    "from scripts.optimize_prompt import optimize_prompt\n",
    "from scripts.evaluation import extract_number, load_math_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.evaluation import extract_number\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def score_exact_match(extracted_prediction, true_answers):\n",
    "    return np.isclose(extracted_prediction, true_answers, rtol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset, fewshot_dataset = load_math_datasets()\n",
    "eval_df, fewshot_df = pd.DataFrame(eval_dataset), pd.DataFrame(fewshot_dataset)\n",
    "\n",
    "OUTPUT_DIR = \"dump\"\n",
    "prompts_file_name = f\"{OUTPUT_DIR}/prompts_gsm8k_mixtral.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup general agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/ubuntu/.cache/huggingface/token\n",
      "Login successful\n",
      "Type:\n",
      "<class 'langchain_community.llms.huggingface_endpoint.HuggingFaceEndpoint'>\n"
     ]
    }
   ],
   "source": [
    "endpoint_url = \"https://fayjubiy2xqn36z0.us-east-1.aws.endpoints.huggingface.cloud\"\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    max_new_tokens=1000,\n",
    ")\n",
    "agent = ChatHuggingFace(llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OKOKOK model::: None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "', i\\'m using the stock 3.5\" lcd on a Sega Saturn and have the whole thing connected to a SNES through a GBS-8200 board, so that i can play Saturn games on the SNES.\\n\\ni\\'ve been trying to find a way to make the Saturn\\'s menu (the one you access by pressing the \"C\" button on the controller) appear on the SNES.\\n\\nas you may know, the GBS-8200 board is able to detect when a console sends a VGA signal, and it outputs it on a VGA port.\\n\\nso, i was wondering if there\\'s a way to make the Saturn send the VGA signal even when the menu is on.\\n\\ni\\'ve tried to find a way to do it by changing some settings on the Saturn, but so far, i\\'ve had no success.\\n\\ndoes anybody know if it\\'s even possible to do it, and if so, how?\\n\\nthanks in advance!'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup teacher optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "teacher_agent = ChatOpenAI(model=\"gpt-4-1106-preview\")\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     endpoint_url=endpoint_url,\n",
    "#     max_new_tokens=1000,\n",
    "# )\n",
    "# teacher_agent = HuggingFaceEndpoint(repo_id=\"mistralai/Mixtral-8x7B-Instruct-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_df = pd.DataFrame(fewshot_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_prompt = \"\"\"\n",
    "Q: What is the answer to the following math problem? Make sure to give your answer as the final number in the format \"The answer is 42\".\n",
    "- {question}\n",
    "\n",
    "A:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.optimize_prompt import optimize_prompt\n",
    "\n",
    "logs = []\n",
    "\n",
    "logs = optimize_prompt(\n",
    "    logs,\n",
    "    initial_prompt,\n",
    "    fewshot_df,\n",
    "    student_agent=agent,\n",
    "    teacher_agent=teacher_agent,\n",
    "    scoring_function=score_exact_match,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_logs(logs, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(logs, f)\n",
    "\n",
    "\n",
    "file_path = f\"{OUTPUT_DIR}/optimizer_mixtral_gpt4-teach.pkl\"\n",
    "if not os.path.exists(file_path):\n",
    "    save_logs(logs, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "logs = pickle.load(open(file_path, \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_best_prompt = max(enumerate(logs), key=(lambda x: x[1][\"score\"]))[0]\n",
    "best_prompt = logs[index_best_prompt][\"prompt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('\\nQ: What is the answer to the following math problem? Make sure to give your answer as the final number in the format \"The answer is 42\".\\n- {question}\\n\\nA:',\n",
       "  0.7),\n",
       " ('\\n\\nQ: Consider the following math problem and solve it step by step. Begin by identifying the given quantities and the relationships between them. Then, use logical reasoning and arithmetic operations to find the final answer. Ensure that you provide a clear and complete explanation of your reasoning process, and conclude with the final answer in the format \"The answer is {final_number}\". Here is the problem:\\n- {question}\\n\\n',\n",
       "  0.9),\n",
       " ('\\nQ: To solve the following math problem, start by enumerating all the given information and the relationships between the different elements involved. Make sure to derive any implied quantities by using logical deductions based on the given data. Carefully check each step to ensure that all information has been considered and used appropriately. Provide a detailed explanation of the reasoning process, and conclude with the final answer in the format \"The answer is {final_number}\". Avoid making assumptions that are not supported by the information provided. Here is the problem:\\n- {question}\\n\\nA:\\n',\n",
       "  0.7),\n",
       " ('\\n\\n\\'When answering the following math problem, take the time to carefully analyze all the provided information. Make sure to account for every detail and the connections between different elements. Your explanation should include each step of your logical reasoning, ensuring that no part of the question is overlooked or misunderstood. Double-check the final number to confirm that it aligns with the given data. Present your step-by-step solution and conclude with the final answer in the format \"The answer is {final_number}\". Keep in mind that precision in reasoning and calculation is key to arriving at the correct conclusion. Here is the problem:\\n- {question}\\n\\n',\n",
       "  0.9),\n",
       " ('\\nQ: Analyze the following math problem carefully. Begin by listing all known quantities and their relationships. Use deductive reasoning to infer any necessary but unstated information that can be logically derived from what is known. Apply mathematical operations step by step to calculate the answer. Throughout your solution process, ensure that every piece of provided information is considered and used accurately. If any assumptions are needed, state them clearly and justify their use based on the given data. Make sure to validate the reasoning and calculations at each step to confirm the consistency and correctness of your solution. Conclude your response by clearly stating the final answer in the format \"The answer is {final_number}\". Here is the problem:\\n- {question}\\n',\n",
       "  0.8),\n",
       " ('\\n\\'Q: Let\\'s work through the following math problem together. Begin by clearly identifying all the specific information provided. Then, apply logical reasoning to connect the given details. Perform the necessary arithmetic calculations step by step to find the solution. Make sure to explain your thought process in a concise manner. Conclude by presenting the final answer in the following format: \"The final answer is {final_number}.\" Do not make any assumptions beyond the facts presented in the question. If the problem requires making an educated guess, state it explicitly and justify your choice. Here is the problem:\\n- {question}\\n\\'\\n',\n",
       "  0.9)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(el[\"prompt\"], el[\"score\"]) for el in logs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test all prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_dict = {\n",
    "    \"initial_prompt\": {\"prompt\": initial_prompt},\n",
    "    \"best_prompt\": {\"prompt\": best_prompt},\n",
    "    \"CoT\": {\"prompt\": \"Q: {question}\\n\\nA: Let's think step-by-step.\"},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fewshot_prompt = \"\"\"\n",
    "Please answer the following math problem. Make sure to give your answer ending with your answer in the format \"The answer is 42\".\n",
    "Here are a few examples to help you.\n",
    "\"\"\"\n",
    "for example in eval_dataset.select(range(3)):\n",
    "    fewshot_prompt += f\"\"\"\n",
    "Q: {example['question']}\n",
    "A: {example['true_reasoning'] + '. So the answer is ' + str(example['true_answer'])}\n",
    "\"\"\"\n",
    "fewshot_prompt += \"Now begin!\\nQ: {question}\\n\\nA:\"\n",
    "fewshot_cot_prompt = fewshot_prompt + \"Let's think step-by-step.\"\n",
    "\n",
    "prompt_dict[\"fewshot\"] = {\"prompt\": fewshot_prompt}\n",
    "prompt_dict[\"fewshot_cot\"] = {\"prompt\": fewshot_cot_prompt}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.optimize_prompt import get_answers\n",
    "\n",
    "for prompt_name, values in prompt_dict.items():\n",
    "    if \"score\" not in values:\n",
    "        prompt = values[\"prompt\"]\n",
    "        print(f\"========== Prompt: {prompt_name} ==========\")\n",
    "        print(f\"Prompt content: {prompt}\")\n",
    "        answers = get_answers(prompt, agent, eval_df[\"question\"])\n",
    "        extracted_answers = answers.apply(extract_number)\n",
    "        answer_match = score_exact_match(extracted_answers, eval_df[\"true_answer\"])\n",
    "        prompt_dict[prompt_name][\"answers\"] = answers.to_dict()\n",
    "        prompt_dict[prompt_name][\"score\"] = answer_match.mean()\n",
    "        print(answer_match.mean())\n",
    "\n",
    "prompt_dict[\"langchain_agent\"] = {\n",
    "    \"prompt\": \"Cf source file\",\n",
    "    \"answers\": \"cf other experiment\",\n",
    "    \"score\": 0.73,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "if not os.path.exists(prompts_file_name):\n",
    "    with open(prompts_file_name, \"w\") as f:\n",
    "        json.dump(prompt_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_mistral = \"dump/prompts_gsm8k_4_mistral7b.json\"\n",
    "file_name_mixtral = \"dump/prompts_gsm8k_mixtral.json\"\n",
    "\n",
    "prompt_dict_mistral = json.load(open(file_name_mistral, \"r\"))\n",
    "prompt_dict_mixtral = json.load(open(file_name_mixtral, \"r\"))\n",
    "\n",
    "results_df_mistral = pd.DataFrame(\n",
    "    [\n",
    "        {**{\"prompt_name\": key, \"model\": \"mistral-7b\"}, **value}\n",
    "        for key, value in prompt_dict_mistral.items()\n",
    "    ]\n",
    ")\n",
    "results_df_mixtral = pd.DataFrame(\n",
    "    [\n",
    "        {**{\"prompt_name\": key, \"model\": \"mixtral-8x7b\"}, **value}\n",
    "        for key, value in prompt_dict_mixtral.items()\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat([results_df_mistral, results_df_mixtral])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model         prompt_name    \n",
       "mistral-7b    CoT                0.200000\n",
       "              best_prompt        0.266667\n",
       "              fewshot            0.200000\n",
       "              fewshot_cot        0.400000\n",
       "              initial_prompt     0.066667\n",
       "              langchain_agent    0.200000\n",
       "mixtral-8x7b  CoT                0.700000\n",
       "              best_prompt        0.666667\n",
       "              fewshot            0.666667\n",
       "              fewshot_cot        0.700000\n",
       "              initial_prompt     0.733333\n",
       "              langchain_agent    0.730000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df.groupby([\"model\", \"prompt_name\"])[\"score\"].first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from the experiment\n",
    "- Prompt optimization with GPT4 does not seem to work well for now 🚫\n",
    "- Prompting techniques are most important for less powerful models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agents",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
